# êµìœ¡ ë‰´ìŠ¤ í¬ë¡¤ë§ ëª¨ë“ˆ (ê°œì„ ëœ ë²„ì „)
import requests
from bs4 import BeautifulSoup
from datetime import datetime
import time
import re
from urllib.parse import urljoin, urlparse
import logging
from smart_filter import SmartNewsFilter
from error_handler import error_handler, log_performance
import concurrent.futures
from typing import List, Dict, Optional
import hashlib

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class EducationNewsCrawler:
    def __init__(self, max_workers: int = 3, timeout: int = 15):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        self.smart_filter = SmartNewsFilter()
        self.max_workers = max_workers
        self.timeout = timeout
        self.crawled_urls = set()  # í¬ë¡¤ë§ëœ URL ìºì‹œ
        self.performance_stats = {
            'total_crawled': 0,
            'successful_crawls': 0,
            'failed_crawls': 0,
            'average_response_time': 0
        }
    
    def extract_clean_title(self, text):
        """ê¹”ë”í•œ ì œëª©ë§Œ ì¶”ì¶œ (ì›ë³¸ ì œëª© ìµœëŒ€í•œ ë³´ì¡´)"""
        if not text:
            return ""
        
        import re
        
        # ìµœì†Œí•œì˜ ì •ë¦¬ë§Œ ìˆ˜í–‰ - ì›ë³¸ ì œëª© ìµœëŒ€í•œ ë³´ì¡´
        # 1. ì—°ì†ëœ ê³µë°± ì •ë¦¬ë§Œ
        text = re.sub(r'\s+', ' ', text).strip()
        
        # 2. ê¸°ìëª…ë§Œ ì œê±° (ì˜ˆ: "ì—„ì„±ìš© ê¸°ì", "í•œë³‘ê·œ ê¸°ì")
        text = re.sub(r'\s*[ê°€-í£]{2,4}\s*ê¸°ì\s*$', '', text)
        
        # 3. ë‚ ì§œ/ì‹œê°„ ì œê±° (ì˜ˆ: "2025-10-20 16:54", "16:54", "14:17")
        text = re.sub(r'\s*\d{4}-\d{2}-\d{2}\s*\d{2}:\d{2}\s*$', '', text)
        text = re.sub(r'\s*\d{2}:\d{2}\s*$', '', text)
        
        # 4. ìµœì¢… ì •ë¦¬
        text = re.sub(r'\s+', ' ', text).strip()
        
        # 5. ë„ˆë¬´ ì§§ê±°ë‚˜ ì˜ë¯¸ì—†ëŠ” ì œëª©ë§Œ ì œê±°
        if len(text) < 5 or text in ['', '...', 'ì œëª©', 'ë‰´ìŠ¤', 'ê¸°ì‚¬']:
            return ""
        
        return text
    
    def normalize_title(self, title):
        """ì œëª© ì •ê·œí™” (ì¤‘ë³µ ì²´í¬ìš©)"""
        import re
        
        # ì†Œë¬¸ì ë³€í™˜, ê³µë°± ì •ë¦¬, íŠ¹ìˆ˜ë¬¸ì ì œê±°
        normalized = re.sub(r'[^\w\sê°€-í£]', '', title.lower())
        normalized = re.sub(r'\s+', ' ', normalized).strip()
        
        # ë¶ˆí•„ìš”í•œ ë‹¨ì–´ ì œê±°
        stop_words = ['ê¸°ì‚¬', 'ë‰´ìŠ¤', 'ë³´ë„', 'ë°œí‘œ', 'ì‹œí–‰', 'ì‹¤ì‹œ']
        for word in stop_words:
            normalized = normalized.replace(word, '')
        
        return normalized
    
    def is_similar_title(self, title, seen_titles):
        """ìœ ì‚¬í•œ ì œëª©ì¸ì§€ í™•ì¸"""
        normalized = self.normalize_title(title)
        
        for seen_title in seen_titles:
            # 80% ì´ìƒ ì¼ì¹˜í•˜ë©´ ìœ ì‚¬í•œ ê²ƒìœ¼ë¡œ íŒë‹¨
            if self.calculate_similarity(normalized, seen_title) > 0.8:
                return True
        return False
    
    def calculate_similarity(self, text1, text2):
        """ë‘ í…ìŠ¤íŠ¸ì˜ ìœ ì‚¬ë„ ê³„ì‚° (ê°„ë‹¨í•œ ë²„ì „)"""
        if not text1 or not text2:
            return 0
        
        # ê³µí†µ ë‹¨ì–´ ìˆ˜ ê³„ì‚°
        words1 = set(text1.split())
        words2 = set(text2.split())
        
        if not words1 or not words2:
            return 0
        
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union) if union else 0
        
    def crawl_education_ministry(self, url, base_url):
        """êµìœ¡ë¶€ ë‰´ìŠ¤ í¬ë¡¤ë§"""
        news_list = []
        try:
            response = self.session.get(url)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # êµìœ¡ë¶€ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ íŒŒì‹±
            news_items = soup.find_all('tr', class_='board-list')
            
            for item in news_items:
                try:
                    title_elem = item.find('td', class_='title')
                    date_elem = item.find('td', class_='date')
                    link_elem = item.find('a')
                    
                    if title_elem and link_elem:
                        title = title_elem.get_text(strip=True)
                        date_text = date_elem.get_text(strip=True) if date_elem else ''
                        link = urljoin(base_url, link_elem.get('href', ''))
                        
                        # ìƒì„¸ ë‚´ìš© í¬ë¡¤ë§
                        content = self.get_article_content(link)
                        
                        news_list.append({
                            'ë‚ ì§œ': date_text,
                            'ì œëª©': title,
                            'ë‚´ìš©': content,
                            'ì¶œì²˜': 'êµìœ¡ë¶€',
                            'ë§í¬': link,
                            'í¬ë¡¤ë§ì‹œê°„': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                        })
                        
                except Exception as e:
                    logger.error(f"êµìœ¡ë¶€ ë‰´ìŠ¤ ì•„ì´í…œ íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue
                    
        except Exception as e:
            logger.error(f"êµìœ¡ë¶€ ë‰´ìŠ¤ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            
        return news_list
    
    def crawl_general_news(self, url, base_url, source_name):
        """ì¼ë°˜ êµìœ¡ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ (ê°œì„ ëœ ë²„ì „)"""
        news_list = []
        seen_titles = set()  # ì¤‘ë³µ ì²´í¬ìš© ì œëª© ì§‘í•©
        seen_links = set()   # ì¤‘ë³µ ì²´í¬ìš© ë§í¬ ì§‘í•©
        
        try:
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            
            logger.info(f"{source_name} í¬ë¡¤ë§ ì‹œì‘ - URL: {url}")
            
            # ì‚¬ì´íŠ¸ë³„ íŠ¹í™”ëœ íŒŒì‹± ë¡œì§ - ì œëª© íƒœê·¸ ì§ì ‘ ì¶”ì¶œ
            if 'eduhope' in source_name.lower():
                # êµìœ¡í¬ë§ íŠ¹í™” íŒŒì‹± - ì œëª© íƒœê·¸ ì§ì ‘ ì°¾ê¸°
                title_elements = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'], class_=re.compile(r'title|head|subject'))
                if not title_elements:
                    # ì œëª© íƒœê·¸ê°€ ì—†ìœ¼ë©´ ë§í¬ì—ì„œ ì¶”ì¶œ
                    all_links = soup.find_all('a', href=True)
                    all_links = [link for link in all_links if link.get_text(strip=True) and len(link.get_text(strip=True)) > 10]
                else:
                    all_links = title_elements
            elif 'hangyo' in source_name.lower():
                # í•œêµ­êµìœ¡ì‹ ë¬¸ íŠ¹í™” íŒŒì‹± - ë” ì •êµí•˜ê²Œ
                # 1. ì œëª© íƒœê·¸ ì°¾ê¸° (ë” ë„“ì€ ë²”ìœ„)
                title_elements = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
                # 2. í´ë˜ìŠ¤ëª…ìœ¼ë¡œ ì œëª© ì°¾ê¸°
                title_by_class = soup.find_all(['div', 'span', 'p'], class_=re.compile(r'title|head|subject|news'))
                # 3. ë§í¬ì—ì„œ ì œëª© ì°¾ê¸°
                link_titles = soup.find_all('a', href=True)
                
                # ëª¨ë“  ë°©ë²•ì„ ì‹œë„
                all_links = []
                if title_elements:
                    all_links.extend(title_elements)
                if title_by_class:
                    all_links.extend(title_by_class)
                if not all_links:
                    all_links = [link for link in link_titles if link.get_text(strip=True) and len(link.get_text(strip=True)) > 10]
            elif 'educhang' in source_name.lower():
                # êµìœ¡ì–¸ë¡ ì°½ íŠ¹í™” íŒŒì‹±
                title_elements = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
                if not title_elements:
                    all_links = soup.find_all('a', href=True)
                else:
                    all_links = title_elements
            else:
                # ì¼ë°˜ì ì¸ íŒŒì‹± - ì œëª© íƒœê·¸ ìš°ì„ , ì—†ìœ¼ë©´ ë§í¬
                title_elements = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
                if title_elements:
                    all_links = title_elements
                else:
                    all_links = soup.find_all('a', href=True)
            
            logger.info(f"{source_name}ì—ì„œ ì´ {len(all_links)}ê°œ ë§í¬ ë°œê²¬")
            
            news_count = 0
            for link in all_links:
                try:
                    text = link.get_text(strip=True)
                    
                    # ì œëª© íƒœê·¸ì¸ ê²½ìš° ë§í¬ ì°¾ê¸°
                    if link.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                        # ì œëª© íƒœê·¸ ì•ˆì˜ ë§í¬ ì°¾ê¸°
                        href_element = link.find('a', href=True)
                        if href_element:
                            href = href_element.get('href', '')
                        else:
                            # ì œëª© íƒœê·¸ ìì²´ì— ë§í¬ê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ
                            continue
                    else:
                        # ì¼ë°˜ ë§í¬ì¸ ê²½ìš°
                        href = link.get('href', '')
                    
                    # ìŠ¤ë§ˆíŠ¸ í•„í„°ë¡œ ë‰´ìŠ¤ì¸ì§€ í™•ì¸
                    if self.smart_filter.is_valid_news(text, href):
                        
                        # ë§í¬ ì™„ì„±
                        full_link = urljoin(base_url, href)
                        
                        # ì›ë³¸ í…ìŠ¤íŠ¸ ë¡œê¹… (ë””ë²„ê¹…ìš©)
                        logger.info(f"ì›ë³¸ í…ìŠ¤íŠ¸: {text[:100]}...")
                        
                        # ê¹”ë”í•œ ì œëª© ì¶”ì¶œ
                        clean_title = self.extract_clean_title(text)
                        
                        # ì¶”ì¶œëœ ì œëª© ë¡œê¹… (ë””ë²„ê¹…ìš©)
                        logger.info(f"ì¶”ì¶œëœ ì œëª©: {clean_title}")
                        
                        # ì œëª©ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ì˜ë¯¸ì—†ìœ¼ë©´ ìŠ¤í‚µ (ë” ì—„ê²©í•˜ê²Œ)
                        if (len(clean_title) < 10 or 
                            clean_title in ['', '...', 'ì œëª©', 'ë‰´ìŠ¤', 'ê¸°ì‚¬'] or
                            clean_title.startswith(('http', 'www', 'mailto')) or
                            'ì •ì±…' in clean_title or 'ì±…ì„ì' in clean_title):
                            continue
                        
                        # ê°•í™”ëœ ì¤‘ë³µ ì²´í¬ (ì œëª© + ë§í¬ ê¸°ì¤€)
                        title_normalized = self.normalize_title(clean_title)
                        is_duplicate = (
                            title_normalized in seen_titles or 
                            full_link in seen_links or
                            self.is_similar_title(clean_title, seen_titles)
                        )
                        
                        if not is_duplicate:
                            seen_titles.add(title_normalized)
                            seen_links.add(full_link)
                            
                            news_list.append({
                                'ë‚ ì§œ': datetime.now().strftime('%Y-%m-%d'),
                                'ì œëª©': clean_title,
                                'ì¶œì²˜': source_name,
                                'ë§í¬': full_link,
                                'í¬ë¡¤ë§ì‹œê°„': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                            })
                            
                            news_count += 1
                            logger.info(f"{source_name} ë‰´ìŠ¤ ìˆ˜ì§‘: {clean_title[:50]}...")
                            
                            if news_count >= 20:  # ìµœëŒ€ 20ê°œ
                                break
                        else:
                            logger.info(f"{source_name} ì¤‘ë³µ ì œì™¸: {clean_title[:30]}...")
                            
                except Exception as e:
                    continue
                    
        except Exception as e:
            logger.error(f"{source_name} ë‰´ìŠ¤ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            
        logger.info(f"{source_name}ì—ì„œ ì´ {len(news_list)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘")
        return news_list
    


    
    @log_performance
    def crawl_all_sources(self, sources: List[Dict]) -> List[Dict]:
        """ëª¨ë“  ë‰´ìŠ¤ ì†ŒìŠ¤ í¬ë¡¤ë§ (ë³‘ë ¬ ì²˜ë¦¬ ê°œì„ )"""
        all_news = []
        
        # ë³‘ë ¬ í¬ë¡¤ë§ ì‹¤í–‰
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # ê° ì†ŒìŠ¤ë³„ í¬ë¡¤ë§ ì‘ì—… ì œì¶œ
            future_to_source = {
                executor.submit(self._crawl_single_source, source): source 
                for source in sources
            }
            
            # ê²°ê³¼ ìˆ˜ì§‘
            for future in concurrent.futures.as_completed(future_to_source):
                source = future_to_source[future]
                try:
                    news = future.result()
                    if news:
                        all_news.extend(news)
                        logger.info(f"âœ… {source['name']}: {len(news)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘")
                    else:
                        logger.warning(f"âš ï¸ {source['name']}: ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨")
                except Exception as e:
                    error_handler.handle_error(
                        e, 
                        f"ì†ŒìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨: {source['name']}",
                        source=source['name'],
                        url=source['url']
                    )
                    logger.error(f"âŒ {source['name']} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        
        # ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸
        self.performance_stats['total_crawled'] += len(all_news)
        self.performance_stats['successful_crawls'] += len(all_news)
        
        return all_news
    
    @error_handler.retry_on_error(max_retries=2, delay=1.0)
    def _crawl_single_source(self, source: Dict) -> List[Dict]:
        """ë‹¨ì¼ ì†ŒìŠ¤ í¬ë¡¤ë§ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)"""
        logger.info(f"ğŸ”„ {source['name']} í¬ë¡¤ë§ ì‹œì‘...")
        
        if 'êµìœ¡ë¶€' in source['name']:
            news = self.crawl_education_ministry(source['url'], source['base_url'])
        else:
            news = self.crawl_general_news(source['url'], source['base_url'], source['name'])
        
        # ìŠ¤ë§ˆíŠ¸ í•„í„° ì ìš©
        filtered_news = self.smart_filter.filter_news_list(news)
        logger.info(f"ğŸ“Š {source['name']}: {len(news)}ê°œ â†’ {len(filtered_news)}ê°œ (í•„í„°ë§ í›„)")
        
        return filtered_news
    
    def generate_content_hash(self, content: str) -> str:
        """ì½˜í…ì¸  í•´ì‹œ ìƒì„± (ì¤‘ë³µ ì²´í¬ìš©)"""
        return hashlib.md5(content.encode('utf-8')).hexdigest()
    
    def enhanced_deduplication(self, news_list: List[Dict]) -> List[Dict]:
        """í–¥ìƒëœ ì¤‘ë³µ ì œê±° (ì œëª© + ë‚´ìš© í•´ì‹œ ê¸°ë°˜)"""
        seen_hashes = set()
        unique_news = []
        
        for news in news_list:
            # ì œëª©ê³¼ ë‚´ìš©ì„ ê²°í•©í•œ í•´ì‹œ ìƒì„±
            content = f"{news.get('ì œëª©', '')}{news.get('ë‚´ìš©', '')}"
            content_hash = self.generate_content_hash(content)
            
            if content_hash not in seen_hashes:
                seen_hashes.add(content_hash)
                unique_news.append(news)
            else:
                logger.debug(f"ì¤‘ë³µ ì œê±°: {news.get('ì œëª©', '')[:30]}...")
        
        logger.info(f"ì¤‘ë³µ ì œê±°: {len(news_list)} â†’ {len(unique_news)}ê°œ")
        return unique_news
    
    def validate_news_quality(self, news: Dict) -> bool:
        """ë‰´ìŠ¤ í’ˆì§ˆ ê²€ì¦"""
        # í•„ìˆ˜ í•„ë“œ í™•ì¸
        required_fields = ['ì œëª©', 'ì¶œì²˜', 'ë§í¬']
        for field in required_fields:
            if not news.get(field):
                return False
        
        # ì œëª© ê¸¸ì´ í™•ì¸ (ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸´ ê²½ìš° ì œì™¸)
        title = news.get('ì œëª©', '')
        if len(title) < 5 or len(title) > 200:
            return False
        
        # ë§í¬ ìœ íš¨ì„± í™•ì¸
        link = news.get('ë§í¬', '')
        if not link.startswith(('http://', 'https://')):
            return False
        
        return True
    
    def get_performance_stats(self) -> Dict:
        """ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
        return {
            **self.performance_stats,
            'error_stats': error_handler.get_error_stats(),
            'crawled_urls_count': len(self.crawled_urls)
        }
    
    def save_to_json(self, news_list: List[Dict], filename: str = 'education_news.json') -> bool:
        """JSON íŒŒì¼ë¡œ ì €ì¥ (ê°œì„ ëœ ë²„ì „)"""
        try:
            if not news_list:
                logger.warning("ì €ì¥í•  ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            # ë°ì´í„° í’ˆì§ˆ ê²€ì¦
            valid_news = [news for news in news_list if self.validate_news_quality(news)]
            logger.info(f"í’ˆì§ˆ ê²€ì¦: {len(news_list)} â†’ {len(valid_news)}ê°œ")
            
            # ì¤‘ë³µ ì œê±°
            unique_news = self.enhanced_deduplication(valid_news)
            
            # JSON ì €ì¥
            import json
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(unique_news, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… JSON íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filename} ({len(unique_news)}ê°œ ë‰´ìŠ¤)")
            return True
            
        except Exception as e:
            error_handler.handle_error(e, "JSON íŒŒì¼ ì €ì¥ ì‹¤íŒ¨", url=filename)
            return False

if __name__ == "__main__":
    from config import NEWS_SOURCES
    
    crawler = EducationNewsCrawler()
    news_list = crawler.crawl_all_sources(NEWS_SOURCES)
    crawler.save_to_json(news_list)
    
    print(f"ì´ {len(news_list)}ê°œì˜ êµìœ¡ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")